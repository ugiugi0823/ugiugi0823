
### [**HyunWook jo**](https://www.notion.so/Make-everyone-s-life-more-fun-via-AI-d6a1722a5aee470a95b82cc84c86c998)



üëâResearch Scientist at  [Modulabs DCV LAB](https://modulabs.co.kr/product/lab-9156-2022-11-16-122419/)

[![Anurag's GitHub stats](https://github-readme-stats.vercel.app/api?username=ugiugi0823)](https://github.com/ugiugi0823/github-readme-stats)

<!--Ïù∏Ïä§ÌÉÄ Î°úÍ∑∏
<a href="https://www.instagram.com/wxxk._o/" target="_blank"><img src = "https://img.shields.io/badge/-Instagram-black?logo=Instagram&logoColor=E4405F"></a>
-->





## Recent implementations
- üî• Denoising-Diffusion-GANs
- Diffusion-Tensorflow
- Vision Transformer Cookbook


## Recent accepted papers
- üî• Generator Knows What Discriminator Should Learn in Unconditional GANs ECCV 2022
- Feature Statistics Mixing Regularization for Generative Adversarial Networks CVPR 2022
- Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks ICLR 2022

## Publications
- ‚ö° Rarity Metric : A New Metric to Evaluate the Uncommonness of Synthesized Images Under review
- Learning Input-agnostic Manipulation Directions in StyleGAN with Text Guidance Under review
- Generator Knows What Discriminator Should Learn in Unconditional GANs ECCV 2022
- Feature Statistics Mixing Regularization for Generative Adversarial Networks CVPR 2022
- Generating Videos with Dynamics-aware Implicit Generative Adversarial Networks ICLR 2022
- Exploiting Spatial Dimensions of Latent in GAN for Real-time Image Editing CVPR 2021
- ‚≠ê U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation ICLR 2020







<!--
**ugiugi0823/ugiugi0823** is a ‚ú® _special_ ‚ú® repository because its `README.md` (this file) appears on your GitHub profile.


- üî≠ I‚Äôm currently working on ...
- üå± I‚Äôm currently learning ...
- üëØ I‚Äôm looking to collaborate on ...
- ü§î I‚Äôm looking for help with ...
- üí¨ Ask me about ...
- üì´ How to reach me: ...
- üòÑ Pronouns: ...
- ‚ö° Fun fact: ...
-->
